One of the most fundamental works in the field was by Greff et al. 2016
[4]. Briefly, they showed that the proposed variations of RNN do not
provide any significant improvement in a large scale study compared to
LSTM. Therefore, LSTM is the dominant architecture in RNNs.

Tutorials used:
https://towardsdatascience.com/video-prediction-using-convlstm-with-pytorch-lightning-27b195fd21a2
https://github.com/guillaume-chevalier/seq2seq-signal-prediction/blob/master/seq2seq.ipynb
https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-2/
https://theaisummer.com/understanding-lstm/

https://arxiv.org/pdf/1308.0850.pdf --> Generating Sequences with RNNs

@article{lstm_a_search_space_odyssey,
  author    = {Klaus Greff and
               Rupesh Kumar Srivastava and
               Jan Koutn{\'{\i}}k and
               Bas R. Steunebrink and
               J{\"{u}}rgen Schmidhuber},
  title     = {{LSTM:} {A} Search Space Odyssey},
  journal   = {CoRR},
  volume    = {abs/1503.04069},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.04069},
  eprinttype = {arXiv},
  eprint    = {1503.04069},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GreffSKSS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
